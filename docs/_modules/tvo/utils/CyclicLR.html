<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>tvo.utils.CyclicLR &mdash; TVO 0.6 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=ec806d2c"></script>
        <script src="../../../_static/doctools.js?v=888ff710"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            TVO
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../tvo.html">tvo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tvo.exp.html">tvo.exp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tvo.models.html">tvo.models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tvo.variational.html">tvo.variational</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tvo.utils.html">tvo.utils</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tvo.utils.parallel.html">tvo.utils.parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tvo.trainer.html">tvo.trainer</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">TVO</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html">Module code</a></li>
      <li class="breadcrumb-item active">tvo.utils.CyclicLR</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for tvo.utils.CyclicLR</h1><div class="highlight"><pre>
<span></span><span class="c1"># This is a backport from pytorch:</span>
<span class="c1"># https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py</span>
<span class="c1">#</span>
<span class="c1"># TODO: Switch to the upstream version when:</span>
<span class="c1"># 1. pytorch makes a release that contains the fix for</span>
<span class="c1">#   https://github.com/pytorch/pytorch/issues/19003, and</span>
<span class="c1"># 2. GOLD supports that pytorch version</span>
<span class="c1">#</span>
<span class="c1"># LICENSING AND COPYRIGHT FOR THIS FILE:</span>
<span class="c1"># See pytorch&#39;s license at https://github.com/pytorch/pytorch/blob/master/LICENSE</span>

<span class="kn">from</span> <span class="nn">torch.optim.optimizer</span> <span class="kn">import</span> <span class="n">Optimizer</span>
<span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">_LRScheduler</span>
<span class="kn">import</span> <span class="nn">math</span>


<div class="viewcode-block" id="CyclicLR">
<a class="viewcode-back" href="../../../tvo.utils.html#tvo.utils.CyclicLR">[docs]</a>
<span class="k">class</span> <span class="nc">CyclicLR</span><span class="p">(</span><span class="n">_LRScheduler</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets the learning rate of each parameter group according to</span>
<span class="sd">    cyclical learning rate policy (CLR). The policy cycles the learning</span>
<span class="sd">    rate between two boundaries with a constant frequency, as detailed in</span>
<span class="sd">    the paper `Cyclical Learning Rates for Training Neural Networks`_.</span>
<span class="sd">    The distance between the two boundaries can be scaled on a per-iteration</span>
<span class="sd">    or per-cycle basis.</span>
<span class="sd">    Cyclical learning rate policy changes the learning rate after every batch.</span>
<span class="sd">    `step` should be called after a batch has been used for training.</span>
<span class="sd">    This class has three built-in policies, as put forth in the paper:</span>
<span class="sd">    &quot;triangular&quot;:</span>
<span class="sd">        A basic triangular cycle w/ no amplitude scaling.</span>
<span class="sd">    &quot;triangular2&quot;:</span>
<span class="sd">        A basic triangular cycle that scales initial amplitude by half each cycle.</span>
<span class="sd">    &quot;exp_range&quot;:</span>
<span class="sd">        A cycle that scales initial amplitude by gamma**(cycle iterations) at each</span>
<span class="sd">        cycle iteration.</span>
<span class="sd">    This implementation was adapted from the github repo: `bckenstler/CLR`_</span>
<span class="sd">    Args:</span>
<span class="sd">        optimizer (Optimizer): Wrapped optimizer.</span>
<span class="sd">        base_lr (float or list): Initial learning rate which is the</span>
<span class="sd">            lower boundary in the cycle for each parameter group.</span>
<span class="sd">        max_lr (float or list): Upper learning rate boundaries in the cycle</span>
<span class="sd">            for each parameter group. Functionally,</span>
<span class="sd">            it defines the cycle amplitude (max_lr - base_lr).</span>
<span class="sd">            The lr at any cycle is the sum of base_lr</span>
<span class="sd">            and some scaling of the amplitude; therefore</span>
<span class="sd">            max_lr may not actually be reached depending on</span>
<span class="sd">            scaling function.</span>
<span class="sd">        step_size_up (int): Number of training iterations in the</span>
<span class="sd">            increasing half of a cycle. Default: 2000</span>
<span class="sd">        step_size_down (int): Number of training iterations in the</span>
<span class="sd">            decreasing half of a cycle. If step_size_down is None,</span>
<span class="sd">            it is set to step_size_up. Default: None</span>
<span class="sd">        mode (str): One of {triangular, triangular2, exp_range}.</span>
<span class="sd">            Values correspond to policies detailed above.</span>
<span class="sd">            If scale_fn is not None, this argument is ignored.</span>
<span class="sd">            Default: &#39;triangular&#39;</span>
<span class="sd">        gamma (float): Constant in &#39;exp_range&#39; scaling function:</span>
<span class="sd">            gamma**(cycle iterations)</span>
<span class="sd">            Default: 1.0</span>
<span class="sd">        scale_fn (function): Custom scaling policy defined by a single</span>
<span class="sd">            argument lambda function, where</span>
<span class="sd">            0 &lt;= scale_fn(x) &lt;= 1 for all x &gt;= 0.</span>
<span class="sd">            If specified, then &#39;mode&#39; is ignored.</span>
<span class="sd">            Default: None</span>
<span class="sd">        scale_mode (str): {&#39;cycle&#39;, &#39;iterations&#39;}.</span>
<span class="sd">            Defines whether scale_fn is evaluated on</span>
<span class="sd">            cycle number or cycle iterations (training</span>
<span class="sd">            iterations since start of cycle).</span>
<span class="sd">            Default: &#39;cycle&#39;</span>
<span class="sd">        cycle_momentum (bool): If ``True``, momentum is cycled inversely</span>
<span class="sd">            to learning rate between &#39;base_momentum&#39; and &#39;max_momentum&#39;.</span>
<span class="sd">            Default: True</span>
<span class="sd">        base_momentum (float or list): Lower momentum boundaries in the cycle</span>
<span class="sd">            for each parameter group. Note that momentum is cycled inversely</span>
<span class="sd">            to learning rate; at the peak of a cycle, momentum is</span>
<span class="sd">            &#39;base_momentum&#39; and learning rate is &#39;max_lr&#39;.</span>
<span class="sd">            Default: 0.8</span>
<span class="sd">        max_momentum (float or list): Upper momentum boundaries in the cycle</span>
<span class="sd">            for each parameter group. Functionally,</span>
<span class="sd">            it defines the cycle amplitude (max_momentum - base_momentum).</span>
<span class="sd">            The momentum at any cycle is the difference of max_momentum</span>
<span class="sd">            and some scaling of the amplitude; therefore</span>
<span class="sd">            base_momentum may not actually be reached depending on</span>
<span class="sd">            scaling function. Note that momentum is cycled inversely</span>
<span class="sd">            to learning rate; at the start of a cycle, momentum is &#39;max_momentum&#39;</span>
<span class="sd">            and learning rate is &#39;base_lr&#39;</span>
<span class="sd">            Default: 0.9</span>
<span class="sd">        last_epoch (int): The index of the last batch. This parameter is used when</span>
<span class="sd">            resuming a training job. Since `step()` should be invoked after each</span>
<span class="sd">            batch instead of after each epoch, this number represents the total</span>
<span class="sd">            number of *batches* computed, not the total number of epochs computed.</span>
<span class="sd">            When last_epoch=-1, the schedule is started from the beginning.</span>
<span class="sd">            Default: -1</span>
<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)</span>
<span class="sd">        &gt;&gt;&gt; scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)</span>
<span class="sd">        &gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)</span>
<span class="sd">        &gt;&gt;&gt; for epoch in range(10):</span>
<span class="sd">        &gt;&gt;&gt;     for batch in data_loader:</span>
<span class="sd">        &gt;&gt;&gt;         train_batch(...)</span>
<span class="sd">        &gt;&gt;&gt;         scheduler.step()</span>
<span class="sd">    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186</span>
<span class="sd">    .. _bckenstler/CLR: https://github.com/bckenstler/CLR</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">base_lr</span><span class="p">,</span>
        <span class="n">max_lr</span><span class="p">,</span>
        <span class="n">step_size_up</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
        <span class="n">step_size_down</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;triangular&quot;</span><span class="p">,</span>
        <span class="n">gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">scale_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">scale_mode</span><span class="o">=</span><span class="s2">&quot;cycle&quot;</span><span class="p">,</span>
        <span class="n">cycle_momentum</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">base_momentum</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
        <span class="n">max_momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">last_epoch</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">Optimizer</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> is not an Optimizer&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>

        <span class="n">base_lrs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_param</span><span class="p">(</span><span class="s2">&quot;base_lr&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">base_lr</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">last_epoch</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">lr</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">base_lrs</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lr</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_param</span><span class="p">(</span><span class="s2">&quot;max_lr&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">max_lr</span><span class="p">)</span>

        <span class="n">step_size_up</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">step_size_up</span><span class="p">)</span>
        <span class="n">step_size_down</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">step_size_down</span><span class="p">)</span> <span class="k">if</span> <span class="n">step_size_down</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">step_size_up</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span> <span class="o">=</span> <span class="n">step_size_up</span> <span class="o">+</span> <span class="n">step_size_down</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span> <span class="o">=</span> <span class="n">step_size_up</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span>

        <span class="k">if</span> <span class="n">mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;triangular&quot;</span><span class="p">,</span> <span class="s2">&quot;triangular2&quot;</span><span class="p">,</span> <span class="s2">&quot;exp_range&quot;</span><span class="p">]</span> <span class="ow">and</span> <span class="n">scale_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mode is invalid and scale_fn is None&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">=</span> <span class="n">mode</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>

        <span class="k">if</span> <span class="n">scale_fn</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;triangular&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_triangular_scale_fn</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">=</span> <span class="s2">&quot;cycle&quot;</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;triangular2&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_triangular2_scale_fn</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">=</span> <span class="s2">&quot;cycle&quot;</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;exp_range&quot;</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_exp_range_scale_fn</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">=</span> <span class="s2">&quot;iterations&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span> <span class="o">=</span> <span class="n">scale_fn</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">=</span> <span class="n">scale_mode</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span> <span class="o">=</span> <span class="n">cycle_momentum</span>
        <span class="k">if</span> <span class="n">cycle_momentum</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;momentum&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">defaults</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;optimizer must support momentum with `cycle_momentum` option enabled&quot;</span>
                <span class="p">)</span>

            <span class="n">base_momentums</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_param</span><span class="p">(</span><span class="s2">&quot;base_momentum&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">base_momentum</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">last_epoch</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">base_momentums</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                    <span class="n">group</span><span class="p">[</span><span class="s2">&quot;momentum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">momentum</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">base_momentums</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">group</span><span class="p">:</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;momentum&quot;</span><span class="p">],</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_momentums</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_param</span><span class="p">(</span><span class="s2">&quot;max_momentum&quot;</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">max_momentum</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">CyclicLR</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">last_epoch</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_format_param</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">param</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return correctly formatted lr/momentum for each param group.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;expected </span><span class="si">{}</span><span class="s2"> values for </span><span class="si">{}</span><span class="s2">, got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">),</span> <span class="n">name</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">param</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">param</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_triangular_scale_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">_triangular2_scale_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">**</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_exp_range_scale_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">**</span> <span class="p">(</span><span class="n">x</span><span class="p">)</span>

<div class="viewcode-block" id="CyclicLR.get_lr">
<a class="viewcode-back" href="../../../tvo.utils.html#tvo.utils.CyclicLR.get_lr">[docs]</a>
    <span class="k">def</span> <span class="nf">get_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculates the learning rate at batch index. This function treats</span>
<span class="sd">        `self.last_epoch` as the last batch index.</span>
<span class="sd">        If `self.cycle_momentum` is ``True``, this function has a side effect of</span>
<span class="sd">        updating the optimizer&#39;s momentum.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">cycle</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_size</span> <span class="o">-</span> <span class="n">cycle</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span><span class="p">:</span>
            <span class="n">scale_factor</span> <span class="o">=</span> <span class="n">x</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scale_factor</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">step_ratio</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">base_lr</span><span class="p">,</span> <span class="n">max_lr</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_lrs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_lrs</span><span class="p">):</span>
            <span class="n">base_height</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_lr</span> <span class="o">-</span> <span class="n">base_lr</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factor</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">==</span> <span class="s2">&quot;cycle&quot;</span><span class="p">:</span>
                <span class="n">lr</span> <span class="o">=</span> <span class="n">base_lr</span> <span class="o">+</span> <span class="n">base_height</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">(</span><span class="n">cycle</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lr</span> <span class="o">=</span> <span class="n">base_lr</span> <span class="o">+</span> <span class="n">base_height</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span>
            <span class="n">lrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_momentum</span><span class="p">:</span>
            <span class="n">momentums</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">base_momentum</span><span class="p">,</span> <span class="n">max_momentum</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_momentums</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_momentums</span><span class="p">):</span>
                <span class="n">base_height</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_momentum</span> <span class="o">-</span> <span class="n">base_momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">scale_factor</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_mode</span> <span class="o">==</span> <span class="s2">&quot;cycle&quot;</span><span class="p">:</span>
                    <span class="n">momentum</span> <span class="o">=</span> <span class="n">max_momentum</span> <span class="o">-</span> <span class="n">base_height</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">(</span><span class="n">cycle</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">momentum</span> <span class="o">=</span> <span class="n">max_momentum</span> <span class="o">-</span> <span class="n">base_height</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">last_epoch</span><span class="p">)</span>
                <span class="n">momentums</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">momentum</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">param_group</span><span class="p">,</span> <span class="n">momentum</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span><span class="p">,</span> <span class="n">momentums</span><span class="p">):</span>
                <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;momentum&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">momentum</span>

        <span class="k">return</span> <span class="n">lrs</span></div>
</div>

</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2019, Machine Learning Group of the University of Oldenburg.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>