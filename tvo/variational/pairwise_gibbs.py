import numpy as np

class PairwiseGibbs(object):
    """
    A sampler that generates samples that roughly follow a multivariate Bernoulli distribution. The behaviour is
    similar to a Gibbs sampler, but rather than considering P(s_new | s_0, ..., s_n) it only considers P(s_new|s_n),
    where n is the index of the last selected state. In other words, it procedurally generates state sequences by
    considering the conditional probability only given the last sampled state.
    """
    def __init__(
        self,
        N: int,
        H: int,
        S: int,
        precision: to.dtype,
        S_new_prior: int,
        S_new_marg: int,
        K_init_file: str = None,
    ):
        """Truncated Variational Sampling class.

        :param N: number of datapoints
        :param H: number of latents
        :param S: number of variational states
        :param precision: floating point precision to be used for log_joint values.
                          Must be one of to.float32 or to.float64.
        :param S_new_prior: number of states to be sampled from prior at every call to ~update
        :param S_new_marg: number of states to be sampled from approximated marginal\
                           p(s_h=1|vec{y}^{(n)}, Theta) at every call to ~update
        :param K_init_file: Full path to H5 file providing initial states
        """
        conf = {
            "N": N,
            "H": H,
            "S": S,
            "S_new_prior": S_new_prior,
            "S_new_marg": S_new_marg,
            "S_new": S_new_prior + S_new_marg,
            "precision": precision,
            "K_init_file": K_init_file,
        }
        super().__init__(conf)
        self.uglyload()

    def sample_init(self, means, covariance_matrices):
        """
        Initializes the PairwiseGibbs.

        Parameters:
        means (array-like): The means of the distribution. Shape: batch x len(s)
        covariance_matrices (array-like): The covariance matrix. Shape: batch x len(s) x len(s)
        """
        means, covariance_matrices=means.cpu().detach().numpy(), covariances.cpu().detach().numpy()

        # TODO: find smart way to process 3d matrices. For now processing only happens on an 1-by-1 basis
        # assert len(means.shape)==2
        # assert len(covariance_matrices.shape)==3
        self.means = means
        self.covariance_matrices = covariance_matrices

        # Convert covariance matrix to pairwise conditional probabilities
        self.pairwise_conditionals = [self.cov2cond(covariance_matrix) for covariance_matrix in covariance_matrices]


        # TODO: verify accuracy AFTER a program is built to do it.

        # Calculate seed probability
        self.seed_probability = self.means / np.sum(self.means)

        # Decide how many bits to select # todo: find a way to compute M. Alternatively, find a way to select states otherwise
        self.expected_value = np.mean(self.means)
        self.variance_of_expected_value = np.sum(np.diag(self.covariance_matrices)) + 2 * np.sum(self.covariance_matrices) - 2 * np.sum(np.outer(self.means, self.means))

    def sample(self, means, covariance_matrices):

        if len(self.means.shape)==1:
            return self._sample()
        else:
            return [self._sample(i) for i in range(self.means.shape[0])]


    def _sample(self,batch):
        # TODO: DO NOT FIND A SMARTER WAY TO DO THIS
        """
        Generates a sample using the custom sampler.

        Returns:
        array: A sample generated by the custom sampler.
        """
        # Select first state
        p=self.means[batch]/self.means[batch].sum()
        s0 = np.random.choice(len(self.means[batch]), p=p)
        bits = [s0]

        # Do M-1 loops
        pwc = self.pairwise_conditionals[batch][s0]
        pwc = pwc / pwc.sum()
        for j in range(self.M()):

            s1 = np.random.choice(len(self.means[batch]), p=pwc)
            bits.append(s1)
            s0 = s1


        vector = np.zeros(self.means.shape[1])
        vector[bits] = 1

        return vector

    def correlation_from_covariance(self, covariance_matrix):
        covariance_matrix = covariance_matrix.reshape(64, 64)
        # assert covariance_matrix.shape[-1]==covariance_matrix.shape[-2]
        # c = covariance_matrix
        # v = np.sqrt(np.diag(c)+abs(np.diag(c).min()))
        # outer_v = np.outer(v, v)
        # correlation = covariance_matrix / outer_v
        # # correlation[covariance_matrix == 0] = 0

        return np.corrcoef(covariance_matrix)

    def conditional_probabilities_from_correlation(self, correlation_matrix):
        conditional_probabilities = np.zeros_like(correlation_matrix)
        for i in range(len(correlation_matrix)):
            for j in range(len(correlation_matrix)):
                if i == j:
                    conditional_probabilities[i, j] = 0
                else:
                    conditional_probabilities[i, j] = correlation_matrix[i, j] / np.sum(correlation_matrix[i]) - \
                                                      correlation_matrix[i, i]
        return conditional_probabilities

    def C2R(self, covariance_matrix):
        return self.correlation_from_covariance(covariance_matrix)

    def R2cond(self, correlation_matrix):
        return self.conditional_probabilities_from_correlation(correlation_matrix)

    def cov2cond(self, covariance_matrix):
        corr = self.C2R(covariance_matrix)
        # cond = self.R2cond(corr)
        norm = (corr+1)/2
        return norm

    def M(self):
        # return int(self.expected_value + np.random.rand()*self.variance_of_expected_value)
        return np.random.randint(1, 10)

    def update(self, idx: to.Tensor, batch: to.Tensor, model: Trainable) -> int:
        """See :func:`tvo.variational.TVOVariationalStates.update`."""
        if isinstance(model, Optimized):
            lpj_fn = model.log_pseudo_joint
            sort_by_lpj = model.sorted_by_lpj
        else:
            lpj_fn = model.log_joint
            sort_by_lpj = {}

        K, lpj = self.K, self.lpj
        batch_size, H = batch.shape[0], K.shape[2]
        lpj[idx] = lpj_fn(batch, K[idx])

        # todo: use neural
        embedding = self.embedder(_inputs)
        means, covariances = self.meaNNs(embedding), self.covariaNNces(embedding)
        self.sample_init(means, covariance_matrices)

        new_K = self.sample()

        new_lpj = lpj_fn(batch, new_K)

        set_redundant_lpj_to_low(new_K, new_lpj, K[idx])

        return update_states_for_batch(new_K, new_lpj, idx, K, lpj, sort_by_lpj=sort_by_lpj)

    def uglyload(self):

        def collect_weights_from_pt(path, name, epoch):
            full_weights_path = '{}/{}_epoch{}.pt'.format(path, name, epoch)
            model = to.load(full_weights_path)
            model.eval()
            return model

        self.embedder = collect_weights_from_pt(target_weights, 'embedder', 1000)
        self.meaNNs = collect_weights_from_pt(target_weights, 'meaNNs', 1000)
        self.covariaNNces = collect_weights_from_pt(target_weights, 'covariaNNces', 1000)



class TVSVariationalStates(TVOVariationalStates):
    def __init__(
        self,
        N: int,
        H: int,
        S: int,
        precision: to.dtype,
        S_new_prior: int,
        S_new_marg: int,
        K_init_file: str = None,
    ):
        """Truncated Variational Sampling class.

        :param N: number of datapoints
        :param H: number of latents
        :param S: number of variational states
        :param precision: floating point precision to be used for log_joint values.
                          Must be one of to.float32 or to.float64.
        :param S_new_prior: number of states to be sampled from prior at every call to ~update
        :param S_new_marg: number of states to be sampled from approximated marginal\
                           p(s_h=1|vec{y}^{(n)}, Theta) at every call to ~update
        :param K_init_file: Full path to H5 file providing initial states
        """
        conf = {
            "N": N,
            "H": H,
            "S": S,
            "S_new_prior": S_new_prior,
            "S_new_marg": S_new_marg,
            "S_new": S_new_prior + S_new_marg,
            "precision": precision,
            "K_init_file": K_init_file,
        }
        super().__init__(conf)

    def update(self, idx: to.Tensor, batch: to.Tensor, model: Trainable) -> int:
        """See :func:`tvo.variational.TVOVariationalStates.update`."""
        if isinstance(model, Optimized):
            lpj_fn = model.log_pseudo_joint
            sort_by_lpj = model.sorted_by_lpj
        else:
            lpj_fn = model.log_joint
            sort_by_lpj = {}

        K, lpj = self.K, self.lpj
        batch_size, H = batch.shape[0], K.shape[2]
        lpj[idx] = lpj_fn(batch, K[idx])



        new_K = self.sample()

        new_lpj = lpj_fn(batch, new_K)

        set_redundant_lpj_to_low(new_K, new_lpj, K[idx])

        return update_states_for_batch(new_K, new_lpj, idx, K, lpj, sort_by_lpj=sort_by_lpj)


